{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "library(spatstat)\n",
    "library(splancs)\n",
    "library(rgeos)\n",
    "library(data.table)\n",
    "library(maptools)\n",
    "\n",
    "#from random.org\n",
    "set.seed(19775046)\n",
    "\n",
    "\n",
    "#set of parameters that run quickly for testing\n",
    "delx=600; dely=600\n",
    "eta=1; lt=14; theta=0\n",
    "features=250; kde.bw=500; \n",
    "kde.lags=6; kde.win = 3\n",
    "call.type='fire'\n",
    "\n",
    "incl_mos = c(10L, 11L, 12L, 1L, 2L, 3L)\n",
    "\n",
    "aa = delx*dely #forecasted area\n",
    "lx = eta*250\n",
    "ly = eta*250\n",
    "\n",
    "#these files created with get_data notebook\n",
    "calls = fread(paste0(call.type, '.csv'))\n",
    "calls[ , date := as.IDate(date)]\n",
    "\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>=\n",
    "# ROTATION ----\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>=\n",
    "\n",
    "#rotation formula, relative to a point (x_0, y_0) that's not origin:\n",
    "#  [x_0, y_0] + R * [x - x_0, y - y_0]\n",
    "#  (i.e., rotate the distances from (x_0, y_0) about that point,\n",
    "#   then offset again by (x_0, y_0))\n",
    "#  Equivalently (implemented below):\n",
    "#  (I - R)[x_0, y_0] + R[x, y]\n",
    "rotate = function(x, y, theta, origin)\n",
    "  matrix(origin, nrow = length(x), \n",
    "         ncol = 2L, byrow = TRUE) %*% (diag(2L) - RT(theta)) + \n",
    "  cbind(x, y) %*% RT(theta)\n",
    "#use the transpose of the rotation matrix to multiply against\n",
    "#  column vectors of coordinates\n",
    "RT = function(theta) matrix(c(cos(theta), -sin(theta), \n",
    "                              sin(theta), cos(theta)), \n",
    "                            nrow = 2L, ncol = 2L)\n",
    "\n",
    "#use the lower-left corner of data as the origin\n",
    "#  through which to rotate\n",
    "#  (side-effect -- the same theta on different\n",
    "#   call types/horizons will result in different\n",
    "#   rotated grids, since point0 will likely differ)\n",
    "point0 = calls[ , c(min(x_lon), min(y_lat))]\n",
    "calls[ , paste0(c('x', 'y'), '_lat') :=\n",
    "          as.data.table(rotate(x_lat, y_lat, theta, point0))]\n",
    "\n",
    "#boundary coordinates of portland,\n",
    "#  as a matrix (rotated immediately)\n",
    "portland = \n",
    "  with(fread('data/portland_coords.csv'),\n",
    "       rotate(x, y, theta, point0))\n",
    "\n",
    "#record range here (after rotation), so that\n",
    "#  we have the same range \n",
    "#  after we subset below\n",
    "#use full boundary range to be sure\n",
    "#  we eventually cover the output polygon\n",
    "xrng = range(portland[ , 1L])\n",
    "yrng = range(portland[ , 2L])\n",
    "\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>=\n",
    "# CREATE GRID ----\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>=\n",
    "\n",
    "getGTindices <- function(gt) {\n",
    "  # Obtain indices to rearange data from image (eg. result frim pixellate)\n",
    "  # so that it conforms with data from GridTopology objects (eg. results\n",
    "  # from using spkernel2d).\n",
    "  # Input: gt is a grid topology.\n",
    "  # Returns an index.\n",
    "  dimx <- gt@cells.dim[1L]\n",
    "  dimy <- gt@cells.dim[2L]\n",
    "  c(matrix(seq_len(dimx*dimy), ncol = dimy, byrow = TRUE)[ , dimy:1L])\n",
    "}\n",
    "\n",
    "# from create GridTopology corresponding to pixel image used for crime counts\n",
    "#   (so that we're sure the output from spatstat & sp overlap properly)\n",
    "grdtop <- as(as.SpatialGridDataFrame.im(\n",
    "  pixellate(ppp(xrange=xrng, yrange=yrng), eps=c(delx, dely))), \"GridTopology\")\n",
    "\n",
    "# index to rearrange rows in pixellate objects\n",
    "idx.new <- getGTindices(grdtop)\n",
    "\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>=\n",
    "# CREATE DATA TABLE OF CRIMES ----\n",
    "# 1) aggregate at lag.window level for included periods\n",
    "# 2) aggregate at forecasting horizon level for included periods\n",
    "# 3) merge previous results\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>=\n",
    "\n",
    "#Before subsetting, get indices of ever-crime cells\n",
    "## Per here, these are always sorted by x,y:\n",
    "##   https://github.com/spatstat/spatstat/issues/37\n",
    "## NB: this must be done within-loop\n",
    "##   since it depends on delx & dely\n",
    "incl_ids = \n",
    "  with(crimes, as.data.table(pixellate(ppp(\n",
    "    #pixellate counts dots over each cell,\n",
    "    #  and appears to do so pretty quickly\n",
    "    x = x_coordina, y = y_coordina,\n",
    "    #ppp complains when given crimes with \n",
    "    #  identical coordinates (which is fine\n",
    "    #  for our purposes), so set check = F\n",
    "    xrange = xrng, yrange = yrng, check = FALSE),\n",
    "    #idx.new here converts from spatstat\n",
    "    #  indexing to GridTopology index order\n",
    "    #  of the grid cells\n",
    "    eps = c(delx, dely)))[idx.new]\n",
    "    #find cells that ever have a crime (value > 0)\n",
    "  )[value > 0, which = TRUE]\n",
    "\n",
    "## Associating each crime with an interval\n",
    "##   of width equal to `horizon`, \n",
    "##   cascading backwards from the \n",
    "##   competition forecasting horizon\n",
    "\n",
    "# how long is one period for this horizon?\n",
    "#   NB: these are not great approximations\n",
    "#       of the horizon lengths, but what is\n",
    "#       crucial is to line up with the\n",
    "#       ultimate forecasting horizons,\n",
    "#       e.g. 1m is March 1 - 31\n",
    "pd_length = switch(horizon, \n",
    "                   '1w' = 7L, '2w' = 14L, '1m' = 31,\n",
    "                   '2m' = 61L, '3m' = 92L) \n",
    "# how many periods are there in one year for this horizon?\n",
    "one_year = switch(horizon, \n",
    "                  '1w' = 52L, '2w' = 26L, '1m' = 12L,\n",
    "                  '2m' = 6L, '3m' = 4L)\n",
    "# how many total periods are there in the data?\n",
    "#   2013/14/15/16/(17), though 17 not used here\n",
    "n_pds = 5L*one_year\n",
    "\n",
    "#actually easier/quicker to deal with\n",
    "#  integer representation of the dates\n",
    "crimes[ , occ_date_int := unclass(occ_date)]\n",
    "#all dates on which a crime occurred\n",
    "#  (basically all dates for crime.type = 'all',\n",
    "#   but substantially curtailed for, e.g., 'burglary')\n",
    "unq_crimes = crimes[ , unique(occ_date_int)]\n",
    "\n",
    "march117 = unclass(as.IDate('2017-03-01'))\n",
    "#all possible period start dates\n",
    "start = march117 - (seq_len(n_pds) - 1L) * pd_length\n",
    "#eliminate irrelevant (summer) data\n",
    "#  and non-testable data after testing dates in 2016\n",
    "start = start[month(as.IDate(start, origin = '1970-01-01')) %in% incl_mos & \n",
    "                start <= march117 - one_year*pd_length]\n",
    "#all period end dates (nonoverlapping with starts)\n",
    "end = start + pd_length - 1L\n",
    "#for feeding to foverlaps\n",
    "windows = data.table(start, end, key = 'start,end')\n",
    "\n",
    "crime_start_map = data.table(occ_date_int = unq_crimes)\n",
    "crime_start_map[ , start_date := \n",
    "                   foverlaps(data.table(start = occ_date_int, \n",
    "                                        end = occ_date_int),\n",
    "                             windows)$start]\n",
    "\n",
    "#finally, add the interval start date associated with\n",
    "#  each date of occurrence\n",
    "crimes[crime_start_map, start_date := i.start_date,\n",
    "       on = 'occ_date_int']\n",
    "\n",
    "#Create the LHS variable -- counts of crimes\n",
    "#  in each cell, in each interval (indexed by start_date)\n",
    "X = crimes[!is.na(start_date), as.data.table(pixellate(ppp(\n",
    "  x = x_coordina, y = y_coordina,\n",
    "  xrange = xrng, yrange = yrng, check = FALSE),\n",
    "  #reorder using GridTopology - im mapping\n",
    "  eps = c(x = delx, dely)))[idx.new],\n",
    "  #subset to eliminate never-crime cells\n",
    "  keyby = start_date][ , I := rowid(start_date)][I %in% incl_ids]\n",
    "\n",
    "#Use four holdout periods -- one for each possible\n",
    "#  year -- to stabilize jumpy prediction validity\n",
    "for (ii in 1:4) {\n",
    "  test_start = march117 - ii * one_year*pd_length\n",
    "  X[start_date <= test_start, \n",
    "    paste0('train_', 17 - ii) := start_date < test_start]\n",
    "}\n",
    "\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>=\n",
    "# Add lagged KDE covariates\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>=\n",
    "\n",
    "# create sp object of crimes\n",
    "to.spdf = function(dt) {\n",
    "  SpatialPointsDataFrame(\n",
    "    coords = dt[ , cbind(x_coordina, y_coordina)],\n",
    "    data = dt[ , -c('x_coordina', 'y_coordina')],\n",
    "    proj4string = CRS(\"+init=epsg:2913\"))\n",
    "}\n",
    "crimes.sp = to.spdf(crimes)\n",
    "#convert to DT to speed up repeated\n",
    "#  subsetting by sorting the data\n",
    "#  to leverage good DT internals\n",
    "crimes.sp@data = setDT(crimes.sp@data)\n",
    "setkey(crimes.sp@data, occ_date_int)\n",
    "\n",
    "#Given a SpatialPointsDF, a start_date,\n",
    "#  and a lag number, calculate the\n",
    "#  KDE corresponding to the kde.win\n",
    "#  days spanning that horizon\n",
    "compute.kde <- function(pts, start, lag.no) {\n",
    "  #subset using data.table for speed;\n",
    "  #  first get the indices via [.data.table\n",
    "  #  of which crimes are included in that lag number\n",
    "  idx = pts@data[occ_date_int %between% \n",
    "                   #between start - kde.win*lag.no\n",
    "                   #  and start - (lag.no - 1)*kde.win - 1\n",
    "                   (start - kde.win*lag.no + c(0, kde.win - 1L)), \n",
    "                 which = TRUE]\n",
    "  #if no crimes found, just return 0\n",
    "  #  (spkernel2d handles this with varying degrees of success)\n",
    "  if (!length(idx)) return(rep(0, length(incl_ids)))\n",
    "  kde = spkernel2d(pts = pts[idx, ],\n",
    "                   #immediately exclude never-crime cells\n",
    "                   poly = portland, h0 = kde.bw, grd = grdtop)[incl_ids]\n",
    "}\n",
    "\n",
    "#start_lag facilitates using within-group lapply\n",
    "#  in the next command, see below\n",
    "start_lag = CJ(start = start, lag = seq_len(kde.lags))\n",
    "\n",
    "#for up through kde.lags total lags to include,\n",
    "#  compute the associated KDE and add it as a\n",
    "#  column associated with the grid cell;\n",
    "#  do this by start_date to complete\n",
    "#  the lagged-KDE specification on the RHS of the model\n",
    "RHS = start_lag[, c(I = list(incl_ids),\n",
    "                    lapply(setNames(lag, paste0('lag', lag)), compute.kde, \n",
    "                           pts = crimes.sp, start = .BY$start)),\n",
    "                by = start]\n",
    "\n",
    "#join to our main data.table\n",
    "X = X[RHS, on = c(start_date = 'start', 'I')]\n",
    "\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# Add Random Fourier Features ----\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\n",
    "#project -- these are the omega * xs\n",
    "proj = X[ , cbind(x, y, start_date)] %*% \n",
    "  (matrix(rt(3L*features, df = 2.5), nrow = 3L)/c(lx, ly, lt))\n",
    "\n",
    "#now convert to data.table to use fwrite,\n",
    "#  also moving into VW-parseable format\n",
    "incl.kde = grep(\"^lag\", setNames(nm = names(X)), value = TRUE)\n",
    "\n",
    "# check for NAs in the features -- still happens\n",
    "#   for long, thin cell sizes since we didn't buffer\n",
    "#   the boundary coordinates enough, and these examples\n",
    "#   end up returning some NA KDE values because of\n",
    "#   how spkernel2d decides which cells are within/not\n",
    "#   the designated boundary coordinates\n",
    "stopifnot(all(!is.na(X$start_date)))\n",
    "\n",
    "phi.dt =\n",
    "  X[ , {\n",
    "    #some nonsense about how get works in j --\n",
    "    #  if we define coln_to_vw in global environment,\n",
    "    #  lapply(incl.kde, coln_to_vw) fails because\n",
    "    #  get doesn't find the variables.\n",
    "    #  Probably some workaround, but w/e\n",
    "    coln_to_vw = function(vn) { \n",
    "      V = get(vn)\n",
    "      #scale up to minimize wasted 0s\n",
    "      #  (eschewing z-scores somewhat arbitrarily)\n",
    "      val = V * 10^(abs(round(mean(log10(V[V>0])))))\n",
    "      #another check on whether we created any\n",
    "      #  missing KDE values -- you might notice\n",
    "      #  this happened a lot, much to our chagrin\n",
    "      if (any(is.nan(val)))\n",
    "        stop('NaNs detected! Current parameters:',\n",
    "             paste(args, collapse = '/'))\n",
    "      sprintf(\"%s:%.5f\", vn, val)\n",
    "    }\n",
    "    c(list(v = value, \n",
    "           #|kdes and |rff define namespaces,\n",
    "           #  which we didn't ultimately use\n",
    "           #  (though could have -- more of\n",
    "           #   a time constraint than anything)\n",
    "           l = paste0(I, \"_\", start_date, \"|kdes\")), \n",
    "      lapply(incl.kde, coln_to_vw),\n",
    "      list(rff_namespace = '|rff'))\n",
    "  }]\n",
    "\n",
    "#about to assign a lot of columns using set --\n",
    "#  will fail if we don't warn data.table that\n",
    "#  phi.dt is going to grow substantially\n",
    "if (features > 500L) invisible(alloc.col(phi.dt, 3L*features))\n",
    "#create the features\n",
    "#  previously explored alternative:\n",
    "#  assign cos/sin projection as matrix:\n",
    "#  phi = cbind(cos(proj), sin(proj))/sqrt(features)\n",
    "#  then assign to phi.dt column-wise,\n",
    "#  but this _appears_ to be slower than implicitly\n",
    "#  creating this as below by taking sin/cos \n",
    "#  simultaneously with assigning to phi.dt.\n",
    "fkt = 1/sqrt(features)\n",
    "for (jj in 1L:features) {\n",
    "  pj = proj[ , jj]\n",
    "  set(phi.dt, j = paste0(c(\"cos\", \"sin\"), jj), \n",
    "      #these are the paired random fourier features;\n",
    "      #  truncating at 5 digits to rein in file size\n",
    "      value = list(sprintf(\"cos%i:%.5f\", jj, fkt*cos(pj)),\n",
    "                   sprintf(\"sin%i:%.5f\", jj, fkt*sin(pj))))\n",
    "}\n",
    "rm(proj)\n",
    "\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# WRITE VW FILES ----\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# vary some things according to the machine on which\n",
    "#   this is being run -- namely:\n",
    "#   tdir - directory for temporary files\n",
    "#   job_id - name of the current job being run\n",
    "#   path_to_vw - where is Vowpal Wabbit?\n",
    "source(\"local_setup.R\")\n",
    "\n",
    "#run over these values of alpha (very fast to do within-loop)\n",
    "alpha_variations = seq(0, 1, length.out = 20)\n",
    "\n",
    "#these are all the indicators for training holdouts\n",
    "train_variations = grep('^train', names(X), value = TRUE)\n",
    "\n",
    "#also relatively fast to recycle through VW\n",
    "#  runs once we have a data set, so\n",
    "#  leverage the fixed cost of data\n",
    "#  creation to do so\n",
    "l1s = l2s = c(0, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3)\n",
    "vw_variations = CJ(l1 = l1s, l2 = l2s)\n",
    "\n",
    "#when we're at the minimum forecast area, we must round up\n",
    "#  to be sure we don't undershoot; when at the max,\n",
    "#  we must round down; otherwise, just round\n",
    "# **TO DO: if we predict any boundary cells and are using the minimum\n",
    "#          forecast area, WE'LL FALL BELOW IT WHEN WE CLIP TO PORTLAND **\n",
    "which.round = function(x)\n",
    "  if (x > 0) {if (x < 1) round else floor} else ceiling\n",
    "\n",
    "#for storing the output, taking care to\n",
    "#  store things in the correct order\n",
    "#  relative to our work flow\n",
    "scores =\n",
    "  CJ(train_set = train_variations,\n",
    "     delx = delx, dely = dely,\n",
    "     alpha = alpha_variations,\n",
    "     eta = eta, lt = lt, theta = theta,\n",
    "     k = features, l1 = l1s, l2 = l2s,\n",
    "     kde.bw = kde.bw, kde.lags = kde.lags,\n",
    "     kde.win = kde.win, pei = 0, pai = 0)\n",
    "\n",
    "#for easy/clean updating syntax\n",
    "setkey(scores, train_set, alpha, l1, l2)\n",
    "\n",
    "#loop over using each year's holdout test set to calculate PEI/PAI\n",
    "for (train in train_variations) {\n",
    "  cat(train, '\\n')\n",
    "  #these are the test data\n",
    "  test_idx = !X[[train]]\n",
    "  \n",
    "  #temporary files -- include lots of\n",
    "  #  potential debugging info in the file name\n",
    "  filename = paste('arws',train,crime.type,horizon,delx,\n",
    "                   dely,eta,lt,theta,features,kde.bw,\n",
    "                   kde.lags,kde.win,job_id,sep = '_')\n",
    "  #write the training data to:\n",
    "  train.vw = paste(paste0(tdir,'/train'), filename, sep='_')\n",
    "  #write the test data to:\n",
    "  test.vw = paste(paste0(tdir,'/test'), filename, sep='_')\n",
    "  #simply append .cache suffix to make it easier\n",
    "  #  to track association when debugging\n",
    "  cache = paste0(train.vw, '.cache')\n",
    "  #write the predictions to:\n",
    "  pred.vw = paste(paste0(tdir,'/pred'), filename, sep='_')\n",
    "  \n",
    "  fwrite(phi.dt[!test_idx], train.vw,\n",
    "         sep = \" \", quote = FALSE, col.names = FALSE,\n",
    "         showProgress = FALSE)\n",
    "  fwrite(phi.dt[test_idx], test.vw,\n",
    "         sep = \" \", quote = FALSE, col.names = FALSE,\n",
    "         showProgress = FALSE)\n",
    "  \n",
    "  # used to delete phi.dt & subset X here to cut down on\n",
    "  #   RAM hit. optionally, could split this into two\n",
    "  #   for loops -- one to write out the files (could then\n",
    "  #   delete phi.dt) and then another to run VW\n",
    "\n",
    "  #Calculate PAI denominator here since it is the\n",
    "  #  same for all variations of tuning parameters,\n",
    "  #  given the input parameters (delx, etc.)\n",
    "  NN = X[test_idx, sum(value)]\n",
    "  \n",
    "  # looping over calls to VW\n",
    "  for (ii in seq_len(nrow(vw_variations))) {\n",
    "    # print(ii)\n",
    "    L1 = vw_variations[ii, l1]\n",
    "    L2 = vw_variations[ii, l2]\n",
    "    #need a new model file for each run\n",
    "    model = tempfile(tmpdir = tdir, pattern = \"model\")\n",
    "    #train with VW\n",
    "    call.vw = paste(path_to_vw, '--loss_function poisson --l1', L1, \n",
    "                    '--l2', L2, train.vw, '--cache_file', cache, \n",
    "                    '--passes 200 -f', model)\n",
    "    #ingore.stderr since this is where system outputs,\n",
    "    #  and having all of the VW output (save for debugging)\n",
    "    #  was just cluttering up the output files\n",
    "    system(call.vw, ignore.stderr = TRUE)\n",
    "    #training data now stored in cache format,\n",
    "    #  so can delete original (don't need to, but this is a useful\n",
    "    #  check to force an error if s.t. wrong with cache)\n",
    "    if (file.exists(train.vw)) invisible(file.remove(train.vw))\n",
    "    #test with VW\n",
    "    system(paste(path_to_vw, '-t -i', model, '-p', pred.vw,\n",
    "                 test.vw, '--loss_function poisson'),\n",
    "           ignore.stderr = TRUE)\n",
    "    invisible(file.remove(model))\n",
    "    \n",
    "    preds =\n",
    "      fread(pred.vw, sep = \" \", header = FALSE, col.names = c(\"pred\", \"I_start\"))\n",
    "    invisible(file.remove(pred.vw))\n",
    "    #wrote 2-variable label with _ to fit VW guidelines;\n",
    "    #  now split back to constituents so we can join\n",
    "    preds[ , c(\"I\", \"start_date\", \"I_start\") :=\n",
    "             c(lapply(tstrsplit(I_start, split = \"_\"), as.integer),\n",
    "               list(NULL))]\n",
    "    \n",
    "    #VW predictions are in logs for poisson regression,\n",
    "    #  so exponentiate to get counts\n",
    "    X[preds, pred.count := exp(i.pred), on = c(\"I\", \"start_date\")]\n",
    "    rm(preds)\n",
    "    \n",
    "    #Calculate all cell rankings, then\n",
    "    #  loop through alpha to include different numbers of them\n",
    "    \n",
    "    #Here are the rankings of the cells according to our predictions\n",
    "    X[X[test_idx, .(tot.pred = sum(pred.count)), by = I\n",
    "        ][order(-tot.pred), .(I, rank = .I)],\n",
    "      rank := i.rank, on = 'I']\n",
    "    \n",
    "    #Here are the TRUE rankings of the cells\n",
    "    X[X[test_idx, .(tot.crimes = sum(value)), by = I\n",
    "        ][order(-tot.crimes), .(I, true_rank = .I)],\n",
    "      true_rank := i.true_rank, on = 'I']\n",
    "    \n",
    "    # loop over alpha values\n",
    "    ##this loop can probably be vectorized better...\n",
    "    for (AA in alpha_variations) {\n",
    "      #6969600 ft^2 = .25 mi^2 (minimum forecast area);\n",
    "      #triple this is maximum forecast area\n",
    "      n.cells = as.integer(which.round(AA)(6969600*(1+2*AA)/aa))\n",
    "      #n.cells = as.integer(ceiling(6969600/aa))\n",
    "\n",
    "      #how well did we do? lower-case n in the PEI/PAI calculation\n",
    "      nn = X[rank <= n.cells & test_idx, sum(value)]\n",
    "      #PEI denominator -- total crimes in the TRUE top n.cells\n",
    "      N_star = X[true_rank <= n.cells & test_idx, sum(value)]\n",
    "      \n",
    "      scores[.(train, AA, L1, L2),\n",
    "             c('pei', 'pai') :=\n",
    "               #pre-calculated the total area of portland (in ft^2)\n",
    "               .(nn/N_star, pai = (nn/NN)/(aa*n.cells/4117777129))]\n",
    "    }\n",
    "    #reset ranking for next run\n",
    "    X[ , rank := NULL]\n",
    "  }\n",
    "  invisible(file.remove(cache, test.vw))\n",
    "}\n",
    "\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# PRINT SCORES TO STDOUT  ====\n",
    "# used when capturing score with Spearmint\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\n",
    "best.pai = scores[ , max(pai), by = train_set][ , mean(V1)]\n",
    "best.pei = scores[ , max(pei), by = train_set][ , mean(V1)]\n",
    "best_scores = paste(best.pai,best.pei, sep = '/')\n",
    "best_scores = paste0('[[[', best_scores,']]]')\n",
    "print(best_scores)\n",
    "\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# WRITE RESULTS FILE \n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\n",
    "ff = paste0(\"scores/\", 'ar_ws_my_',crime.type, \"_\", horizon, job_id, \".csv\")\n",
    "fwrite(scores, ff, append = file.exists(ff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
